# -*- coding: utf-8 -*-
"""UPLOAD EMODETEC Bi-LSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Mti0VT9YVXkdedTf6vQzt7NqyBdLWp6

* <b> Overview :</b> Explore text-based emotion recognition, a dynamic field in <span style="background-color: #F8DE22; padding: 4px; border-radius:5px;">NLP</span>, focusing on deciphering diverse emotional states in textual content.

* <b> Objective :</b> Build a system for automatic categorization of text into six emotions
( <span style="color: #F8DE22;">joy</span>  ,
 <span style="color: #0c0d49;">sadness</span> ,
 <span style="color: #b82f2f;">fear</span> ,
 <span style="color: #331e1e;">anger</span > ,
 <span style="color: red;">love</span> ,
 <span style="color: #00fff7;">surprise</span>)
* <b> Model Choice : </b> Utilize <span style="background-color: #F8DE22; padding: 4px; border-radius:5px;">LSTM</span>
 (Long Short-Term Memory) networks, a type of <span style="background-color: #F8DE22; padding: 4px; border-radius:5px;">RNN</span>.

* <b> Implementation : </b>
Implemented with <span style="background-color: #F8DE22; padding: 4px; border-radius:5px;">TensorFlow</span>.
"""

import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from wordcloud import WordCloud
from nltk.stem import PorterStemmer
import numpy as np
from keras.utils import to_categorical
from sklearn.metrics import confusion_matrix


import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

"""# 1 | The Dataset

* any data sets involving sentiment analysis are <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">binary classification</span> problems
* In this dataset we have <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">6 different sentiments</span> , which means we'll be treating this problem as a <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">multiclass classification</span> problem

### 1. 1 | Loading Data
"""

import pandas as pd
PATH='/content/emotion_dataset.csv'
new_data = pd.read_csv(PATH)

import re

# Function to lowercase and remove usernames
def clean_tweet(tweet):
    # Lowercase the tweet
    cleaned_tweet = tweet.lower()
    # Remove usernames using regex
    cleaned_tweet = re.sub(r'@([A-Za-z0-9_]+)', '', cleaned_tweet)
    cleaned_tweet = cleaned_tweet.replace('#', '')
    return cleaned_tweet

# Apply the function to each row in the 'tweets' column
new_data['text'] = new_data['Text'].apply(clean_tweet)

# Display the result
print(new_data)

# Selecting the desired columns and renaming them
new_clean_data = new_data[['text', 'Emotion']].rename(columns={'text': 'text', 'Emotion': 'label'})

# Define the mapping dictionary
label_mapping = {
    'sadness': 0,
    'joy': 1,
    'anger': 2,
    'fear': 3,
    'surprise': 4,
    'neutral': 5,
    'shame': 6,
    'disgust': 7
}

# Assuming df is your DataFrame and 'label' is the column containing the string labels
new_clean_data['label'] = new_clean_data['label'].map(label_mapping)

from sklearn.model_selection import train_test_split

# Assuming df contains your DataFrame with 'text' and 'label' columns
# Splitting into train (80%) and temp (20%)
train_df, temp_df = train_test_split(new_clean_data, test_size=0.2, random_state=42)

# Splitting temp into validation (50%) and test (50%)
validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Displaying the shapes of the resulting DataFrames
print("Train set shape:", train_df.shape)
print("Validation set shape:", validation_df.shape)
print("Test set shape:", test_df.shape)

val_PATH=''
train_PATH=''
test_PATH=''
val_data = pd.read_csv(val_PATH)
train_data = pd.read_csv(train_PATH)
test_data = pd.read_csv(test_PATH)

print("Validation data :",val_data.shape)
print("Train data :",train_data.shape)
print("Test data :",test_data.shape)

"""* There is a lot of data in test, in my case i divided it and put the est in the val_data"""

half_test_data = test_data.iloc[1000:]
test_data = test_data.iloc[:1000]

val_data = pd.concat([val_data, half_test_data], axis=0)

print("new Vald data :",val_data.shape)
print("new Test data :",test_data.shape)

# Concatenate the DataFrames
train_data = pd.concat([train_data, train_df])
val_data = pd.concat([val_data, validation_df])
test_data = pd.concat([test_data, test_df])

# Displaying the shapes of the resulting DataFrames
print("Train set shape:", train_data.shape)
print("Validation set shape:", val_data.shape)
print("Test set shape:", test_data.shape)

# Shuffle train set
train = train_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffle validation set
validation = val_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffle test set
test = test_data.sample(frac=1, random_state=42).reset_index(drop=True)

train_data.head(10)

"""### 1. 2 | Adding Label Data"""

labels_dict = {0:'sadness', 1:'joy', 2:'love', 3:'anger', 4:'fear', 5:'surprise', 6:'neutral', 7:'shame', 8:'disgust'}
train_data['label_name'] = train_data['label'].map(labels_dict)
train_data.head()

train_data.groupby(["label_name","label"]).size()

"""### 1. 3 | Data Visualization"""

train_data["label_name"].value_counts().plot(kind='bar',color=['yellow', '#0c0d49', '#b82f2f', '#331e1e', 'red','#00fff7'])

"""# 2 | Data Cleaning"""

print(train_data.isnull().sum())
print(val_data.isnull().sum())
print(test_data.isnull().sum())

"""# 3 | Tokenisation & Stemming

* <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">Tokenization</span> assigns unique IDs to words, creating a word index or vocabulary.
* <b>Example Sentence :</b> "Tokenization is essential for NLP tasks."
* <b>Tokenized Output : </b>['Tokenization', 'is', 'essential', 'for', 'NLP', 'tasks', '.']


* <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">Stemming</span> is a technique used to reduce an inflected word down to its word stem.
* <b>Example :</b>
* <b>Original Words :</b> running , programming , swimming , happiness , programmer <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">  (5 words)</span>
* <b>Stemmed Words :</b> run , program , swim , happi   <span style="background-color: #F8DE22; padding: 2px; border-radius:5px;">  (4 words)</span>
"""

all_list = train_data['text'].tolist() + test_data['text'].tolist() + val_data['text'].tolist()

tokenizer1 = Tokenizer()
tokenizer1.fit_on_texts(all_list)
word_index1 = tokenizer1.word_index

print("Nombre of words without Stemming:",len(word_index1))

stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in word_index1.keys()]

tokenizer2 = Tokenizer()
tokenizer2.fit_on_texts(stemmed_words)
word_index2 = tokenizer2.word_index

print("Nombre of words with Stemming:",len(word_index2))

"""* load all data to list : <b>[ [ Tokenised_Data ] , label ] </b>"""

def preprocess_data(data):
    new_data = []
    for index, row in data.iterrows():
        test_split = row['text'].split()
        stemmed_words2 = [stemmer.stem(word) for word in test_split]
        token_list= tokenizer2.texts_to_sequences([stemmed_words2])[0]
        new_data.append([token_list,row['label']])
    return new_data

new_train_data = preprocess_data(train_data)
print(train_data['text'][0])
print(new_train_data[0])

new_val_data = preprocess_data(val_data)
print(val_data['text'][0])
print(new_val_data[0])

# Splitting into train_X and train_y
train_X = [row[0] for row in new_train_data]
train_y = [row[1] for row in new_train_data]

# Print the results
print("train_X:", train_X[0])
print("train_y:", train_y[0])

val_X = [row[0] for row in new_val_data]
val_y = [row[1] for row in new_val_data]

print("train_X:", val_X[0])
print("train_y:", val_y[0])

"""### 3. 2 | Add Padding"""

length_of_longest_sentence = len(max(train_X, key=len))
print(length_of_longest_sentence)
print(max(train_X, key=len))

for i in range(len(train_X)):
    for j in range(length_of_longest_sentence-len(train_X[i])):
        train_X[i].append(0)

for i in range(len(val_X)):
    for j in range(length_of_longest_sentence-len(val_X[i])):
        val_X[i].append(0)

"""### 3. 3 | List to Array (numpy)"""

train_X = np.array(train_X)
train_y = np.array(train_y)
val_X = np.array(val_X)
val_y = np.array(val_y)

print(train_X.shape,train_y.shape)
print(val_X.shape,val_y.shape)

# Convert labels to one-hot encoding
train_y_one_hot = to_categorical(train_y, num_classes=16000)
val_y_one_hot = to_categorical(val_y, num_classes=16000)

"""# 4 | Create model (LSTM)

### 4. 1 | Architechture of Bidirectional LSTM Neural Network

![download.png](attachment:e3218baf-2240-45f3-8aac-e6956af5346e.png)

### 4. 2 | Bi- LSTM Neural Network Model training
"""

model = Sequential()
model.add(Embedding(16000, 100, input_length=66))
model.add(Bidirectional(LSTM(150)))
model.add(Dense(16000, activation='softmax'))
adam = Adam(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
history = model.fit(train_X, train_y_one_hot, epochs=25, verbose=1,validation_data=(val_X,val_y_one_hot))
#print model.summary()
print(model)

"""# 5 | Resultd And Test"""

last_accuracy = "{:.3f}".format(history.history['accuracy'][-1])
print("Training Accuracy:", last_accuracy)

"""### 5. 1 | Plotting Model Accuracy And Loss"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Training Accuracy vs Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training Loss vs Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""### 5. 2 | Test The Model"""

def get_text(text):
    tokenizer3 = Tokenizer()
    tokenizer3.fit_on_texts(text)
    word_index3 = tokenizer3.word_index

    stemmed_wordss = [stemmer.stem(word) for word in word_index3.keys()]

    tokens_list= tokenizer2.texts_to_sequences([stemmed_wordss])[0]

    for i in range(len(tokens_list)):
        for j in range(length_of_longest_sentence-len(tokens_list)):
            tokens_list.append(0)
    return tokens_list

for _ in range(5):
    random_number = random.randint(0, 1000)
    num_to_predicte = random_number

    test = get_text([test_data['text'][num_to_predicte]])

    test = np.array(test)
    test = test.reshape(1, len(test))

    # Make predictions
    predictions = model.predict(test)

    predicted_class = np.argmax(predictions)
    print()
    print('Random value = ',random_number)
    print("Predicted Class:", predicted_class,labels_dict.get(predicted_class))
    print("Actual Class:", test_data['label'][num_to_predicte])
    print()

"""### 5. 2 | Confusion Matrix"""

new_test_data=preprocess_data(test_data)

test_X = [row[0] for row in new_train_data]
test_y = [row[1] for row in new_train_data]

for i in range(len(test_X)):
    for j in range(length_of_longest_sentence-len(test_X[i])):
        test_X[i].append(0)

test_X = np.array(test_X)
test_y = np.array(test_y)

test_y_one_hot = to_categorical(test_y, num_classes=16000)

y_pred = model.predict(test_X)
y_pred_classes = np.argmax(y_pred, axis=1)

y_true_labels = np.argmax(test_y_one_hot, axis=1)

labels=['sadness','joy','love','anger','fear','surprise']
#labels = list(set(labels).intersection(set(np.unique(y_true_labels)).union(set(np.unique(y_pred_classes)))))

cm = confusion_matrix(y_true_labels, y_pred_classes)
df_cm = pd.DataFrame(cm, labels, labels)
ax = sns.heatmap(df_cm, annot=True, annot_kws={'size': 16}, square=True, cbar=False, fmt='g')
ax.set_ylim(0, 6)
plt.xlabel('Predicted')
plt.ylabel('Actual')
ax.invert_yaxis()
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Compute precision, recall, and F1 score for each class
precision = precision_score(y_true_labels, y_pred_classes, average=None)
recall = recall_score(y_true_labels, y_pred_classes, average=None)
f1 = f1_score(y_true_labels, y_pred_classes, average=None)

# Compute macro F1 score
macro_f1 = f1_score(y_true_labels, y_pred_classes, average='macro')

print("Precision per class:", precision)
print("Recall per class:", recall)
print("F1 score per class:", f1)
print("Macro F1 score:", macro_f1)

from sklearn.metrics import matthews_corrcoef

# Compute Matthews Correlation Coefficient
mcc = matthews_corrcoef(y_true_labels, y_pred_classes)

print("Matthews Correlation Coefficient:", mcc)